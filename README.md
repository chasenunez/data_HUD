# DataHUD is a customizable user interface to display your data to the public. This Script:

* displays CSV/TSV tabular data as a beautiful, interactive table,
* supports sorting & filtering,
* allows selection and client-side export (CSV/JSON/XLSX),
* includes the original citation & license metadata in every downloaded file,
* is configurable (logo, header text, fonts, colors) via a small `config.json`,
* has a Python ingestion/validation script to turn CSVs into JSON and generate column metadata for the front end,
* includes an optional GitHub Actions workflow to automate build steps into the `docs/` folder (so GitHub Pages can host it), and
* is thoroughly commented so researchers / RSEs can personalise it.

# Architecture (short)

A. **Static frontend** (HTML+JS+CSS) served by GitHub Pages. Frontend reads `config.json` for branding (logo path, colors, fonts, header text). It fetches the dataset (either `data.json` produced by the Python script or a CSV published in the repo / S3) and renders a Tabulator table with filtering/sorting/export. (GitHub Pages only serves static files — no server-side Python on the site itself).

B. **Python preprocessing script** (`ingest.py`) — run locally or in CI (GitHub Actions). Responsibilities:

   * validate CSV (columns exist, types optional),
   * create `data.json` (fast for browser to load) and `columns.json` (Tabulator column defs),
   * create `metadata.json` (authors/citation/license/DOI),
   * optionally pull CSV from S3 (or any URL) and/or push generated outputs to repo `docs/` (or `gh-pages` branch).

C. **Optional GitHub Actions** workflow to run `ingest.py` on pushes to a `data/` folder and write generated site artifacts to `docs/` so GitHub Pages publishes them automatically. (You can also run the Python script locally and commit the outputs manually.)

# File / folder layout

```
my-data-explorer/
├─ data/                        # (optional) raw CSVs you edit and push
│  └─ my_study_data.csv
├─ scripts/
│  └─ ingest.py                 # Python ingestion + validation script (below)
├─ docs/                        # publishing folder for GitHub Pages (or use gh-pages branch)
│  ├─ index.html                # main UI (below)
│  ├─ css/
│  │  └─ style.css
│  ├─ js/
│  │  └─ app.js
│  ├─ config.json               # site branding + table settings
│  ├─ data.json                 # generated by ingest.py (or you can place CSV and let app parse)
│  ├─ columns.json              # generated by ingest.py (Tabulator column defs)
│  └─ metadata.json             # citation / license info for downloads
├─ requirements.txt             # pandas, boto3 (optional), pyarrow (optional)
└─ README.md
```

# The Python ingestion/validation script (`scripts/ingest.py`)

This script:

* reads a CSV (from local `data/` or S3 URL),
* validates required columns if you set them in `config.json`,
* infers types and build `columns.json` for Tabulator,
* writes `data.json` and `metadata.json` to `docs/` (so the frontend loads quickly).

Saves as `scripts/ingest.py` but you must edit the CONFIG defaults to match your fields.


# Frontend composed of `docs/index.html` + `docs/js/app.js` + `docs/css/style.css`

uses [Tabulator](https://tabulator.info/](https://tabulator.info/) to provide out-of-the-box filtering, sorting, client-side downloads (CSV/JSON/XLSX/PDF), good styling options, and is simple to configure. It’s a robust, widely used grid suited to this use case.


**What the frontend does**

* Loads `config.json`, applies colors/font/logo. (Logo set in `config.json` should be a path relative to `docs/` or an external URL.)
* Loads `metadata.json`, `columns.json`, `data.json` (generated by `ingest.py`).
* Builds Tabulator table with per-column header filters, pagination, and selectable rows.
* Provides quick-search / clear filters controls.
* Exports currently visible rows with the metadata prefixed in CSV as `# key: value` comment lines (human readable & machine parsable), and JSON that includes metadata object.

# Personalization with`config.json`

Place in `docs/config.json`. This is intended to be edited by the repository owner to personalize the site.

```json
{
  "title": "My Research Data Explorer",
  "subtitle": "Interactive preview of study dataset — filter, sort, export",
  "logo": "images/my_logo.png",
  "font": "Inter, Arial, sans-serif",
  "primary_color": "#0b5fff",
  "background_color": "#ffffff",
  "table_options": {
    "paginationSize": 50
  }
}
```

# Citation and metadata data provided with metadata.json

This is generated by the Python script, but you can edit it in `docs/metadata.json`:

```json
{
  "generated_at": "2025-11-05T12:34:00Z",
  "source_filename": "my_study_data.csv",
  "n_rows": 1234,
  "n_columns": 12,
  "citation": "Doe, J. & Smith, A. (2024). Example dataset. Zenodo. https://doi.org/10.5281/zenodo.xxxxxx",
  "license": "CC-BY 4.0",
  "doi": "10.5281/zenodo.xxxxxx",
  "contact": "data-owner@example.edu",
  "notes": "Collected during field season 2022."
}
```

How metadata is included in downloads (key detail)

* CSV download: the JS `buildCSVWithMetadata` prepends lines beginning with `#` containing `citation`, `license`, `doi`, `generated_at`, `exported_at`. This gives a quick human-readable provenance header that many tools will ignore as comments but preserves provenance.
* JSON download: a top-level `metadata` object is included.
* You can change the format easily (e.g., add a separate `README` file per dataset or include a `LICENSE.txt` in `docs/`).
